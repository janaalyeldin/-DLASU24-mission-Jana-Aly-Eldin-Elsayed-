# -*- coding: utf-8 -*-
"""CNN_model2_final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18bU_mycr-HuQeHzmc894qpvOX_9ui2u6
"""

# Commented out IPython magic to ensure Python compatibility.
import torch
from torch.utils.data import TensorDataset, DataLoader
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import torchvision
import numpy as np
import matplotlib.pyplot as plt
# %matplotlib inline

from google.colab import drive
drive.mount('/content/drive')

print(torch.cuda.is_available())

device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')

x=np.load('/content/drive/MyDrive/X_train_tabular.npy')

y=np.load('/content/drive/MyDrive/y_train_tabular.npy')

x_tensor_cnn=torch.tensor(x, dtype=torch.float32)
y_tensor_cnn=torch.tensor(y, dtype=torch.float32)

x_tensor_cnn1=x_tensor_cnn.view(-1,12,1,1)

train_dataset_cnn = TensorDataset(x_tensor_cnn1, y_tensor_cnn)
train_loader_cnn = DataLoader(train_dataset_cnn, batch_size=32, shuffle=True,num_workers=2)

#Model
class CNN1(nn.Module):
  def __init__(self):
    super().__init__()
    self.conv1=nn.Conv2d(12,32,kernel_size=1)
    self.conv2=nn.Conv2d(32,64,kernel_size=1)
    #self.conv3=nn.Conv2d(32,64,2,1,padding=1)
    #fully connected layers
    self.fc1=nn.Linear(1*1*64 ,128)
    self.fc2=nn.Linear(128,64)
    self.fc3=nn.Linear(64,14)

  def forward(self,x):
    x=F.relu(self.conv1(x))
    x=F.max_pool2d(x,1) #2*2 kernel and stride of 2
    x=F.relu(self.conv2(x))
    x=F.max_pool2d(x,1)
    #x=F.relu(self.conv3(x))
    #x=F.max_pool2d(x,2,2)

    #flatten the data
    x=x.reshape(-1,64*1*1)  #-1 so that we can vary the batch size
    #fully connected layer
    x=F.relu(self.fc1(x))
    #x=self.fc2(x)
    x=F.relu(self.fc2(x))
    x=self.fc3(x) #no relu on last one bcuz its the last one
    return x

#create instance of the model
model_cnn=CNN1().to(device)

criterion=nn.MSELoss()
optimizer=optim.Adam(model_cnn.parameters(),lr=0.001)

#create variables to track things
epochs=10
train_losses_cnn1=[]

#for loop for epochs
for epoch in range(epochs):
  trn_corr=0
  #Train
  for (inputs,targets) in (train_loader_cnn):

    #inputs = inputs.view(-1, 12,1,1).to(device)
    inputs = inputs.to(device)
    targets = targets.to(device)
    outputs = model_cnn(inputs)  # Forward pass  #4,14
    #print(outputs.shape)
    #print(targets.shape)
    loss = criterion(outputs, targets)
    # Compute the loss
    optimizer.zero_grad()  # Zero the parameter gradients to avoid accumulation
    loss.backward()  # Backpropagation
    optimizer.step()  #
  print(f"Epoch [{epoch+1}/{epochs}],  Loss: {loss.item():.4f}")
  train_losses_cnn1.append(loss)

train_losses_cnn1 = [tl.cpu().item() for tl in train_losses_cnn1]  # Convert each tensor to a float

plt.plot(train_losses_cnn1, label="Training loss")
plt.title("Loss at Epoch")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.legend()
plt.show()