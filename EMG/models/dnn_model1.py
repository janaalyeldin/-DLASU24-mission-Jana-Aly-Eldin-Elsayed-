# -*- coding: utf-8 -*-
"""DNN_model1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1y3f_ysGZl2Jmtr4cdci3W7Lk3VIvGjc0
"""

# Commented out IPython magic to ensure Python compatibility.
import torch
from torch.utils.data import TensorDataset, DataLoader
import torch.nn as nn
import torch.optim as optim
import torchvision
import numpy as np
import matplotlib.pyplot as plt
# %matplotlib inline

print(torch.cuda.is_available())

device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')

x=np.load('/content/drive/MyDrive/Colab Notebooks/Copy of X_train_tabular.npy')

y=np.load('/content/drive/MyDrive/Colab Notebooks/Copy of y_train_tabular.npy')

X_train_tensor = torch.tensor(x, dtype=torch.float32)
y_train_tensor = torch.tensor(y, dtype=torch.float32)

# Create a dataset and dataloader
train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)

# Hyperparameters
input_size = 12      # 12 features in x_train
hidden_size = 64     # Number of neurons in the hidden layers
output_size = 14     # 14 features in y_train
learning_rate = 0.001
num_epochs = 10
batch_size = 32

class SimpleDNN(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(SimpleDNN, self).__init__()
        self.fc1 = nn.Linear(input_size, hidden_size)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(hidden_size, hidden_size)
        self.fc3 = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        out = self.fc1(x)
        out = self.relu(out)
        out = self.fc2(out)
        out = self.relu(out)
        out = self.fc3(out)
        return out

model_dnn = SimpleDNN(input_size,hidden_size,output_size).to(device)
criterion = nn.MSELoss()
optimizer = optim.Adam(model_dnn.parameters(), lr=0.001)

epochs = 10
train_losses_dnn=[]
# Training loop
for epoch in range(num_epochs):
    for (inputs, targets) in train_loader:
        # Forward pass
        inputs=inputs.to(device=device)
        targets=targets.to(device=device)
        outputs = model_dnn(inputs)
        loss = criterion(outputs, targets)

        # Backward pass and optimization
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')
    train_losses_dnn.append(loss)

train_losses_dnn = [tl.cpu().item() for tl in train_losses_dnn]  # Convert each tensor to a float

plt.plot(train_losses_dnn, label="Training loss")
plt.title("Loss at Epoch")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.legend()
plt.show()