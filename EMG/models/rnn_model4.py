# -*- coding: utf-8 -*-
"""RNN_model4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NlsL_cJ65Ae30wTQs2pDhhDRRGdHZynQ
"""

# Commented out IPython magic to ensure Python compatibility.
import torch
from torch.utils.data import TensorDataset, DataLoader
import torch.nn as nn
import torch.optim as optim
import torchvision
import numpy as np
import matplotlib.pyplot as plt
# %matplotlib inline

print(torch.cuda.is_available())

x_seq=np.load('/content/drive/MyDrive/Colab Notebooks/X_train_padding.npy')

y_seq=np.load('/content/drive/MyDrive/Colab Notebooks/y_train_padding.npy')

x_tensor=torch.from_numpy(x_seq)
y_tensor=torch.from_numpy(y_seq)
x_tensor=x_tensor.float()
y_tensor=y_tensor.float()

device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')

#HYPERPARAMETERS
input_size=12
sequence_length=12246
num_layers=2
hideen_size=256
num_classes=14
learning_rate=0.001
batch_size=32
num_epochs=10

class RNN(nn.Module):
  def __init__(self,input_size,hidden_size,num_layers,num_classes):
    super(RNN,self).__init__()
    self.hidden_size=hidden_size
    self.num_layers=num_layers
    self.rnn=nn.RNN(input_size,hidden_size,num_layers,batch_first=True)
    self.fc=nn.Linear(hidden_size*sequence_length,num_classes)
  def forward(self,x):
    h0=torch.zeros(self.num_layers,x.size(0),self.hidden_size).to(device)
    out,_=self.rnn(x,h0)
    out=out.reshape(out.shape[0],-1)  #256*12246
    out=self.fc(out)
    return out

dataset_rnn=TensorDataset(x_tensor,y_tensor)
dataloader_rnn=DataLoader(dataset_rnn,batch_size=batch_size,shuffle=True)

model_RNN=RNN(input_size=input_size,hidden_size=hideen_size,num_layers=num_layers,num_classes=num_classes).to(device)
criterion=nn.MSELoss()
optimizer=optim.Adam(model_RNN.parameters(),lr=learning_rate)

train_losses=[]
for epoch in range(num_epochs):
  for inputs,targets in (dataloader_rnn):
    inputs=inputs.to(device=device)
    targets=targets.to(device=device)
    #data=data.to(device=device).squeeze(1)
    #targets=targets.to(device=device).squeeze(1)

    scores=model_RNN(inputs)
    targets = targets[:, -1, :]
    #targets = torch.argmax(targets, dim=1)
    #targets=targets.float()
    loss=criterion(scores,targets)

    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

  print(f"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}")
  train_losses.append(loss)

train_losses = [tl.cpu().item() for tl in train_losses] # Convert generator to list
plt.plot(train_losses, label="Training loss")
plt.title("Loss at Epoch")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.legend()
plt.show()