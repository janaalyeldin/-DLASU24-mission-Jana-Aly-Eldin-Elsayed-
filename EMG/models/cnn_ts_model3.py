# -*- coding: utf-8 -*-
"""CNN_TS_model3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qObej5ChazucZEVUGnr-T35EnJhdhTA7
"""

# Commented out IPython magic to ensure Python compatibility.
import torch
from torch.utils.data import TensorDataset, DataLoader
import torch.nn as nn
import torch.optim as optim
import torchvision
import numpy as np
import matplotlib.pyplot as plt
# %matplotlib inline

x_seq=np.load('/content/drive/MyDrive/Colab Notebooks/X_train_padding.npy')

y_seq=np.load('/content/drive/MyDrive/Colab Notebooks/y_train_padding.npy')

x_tensor_ts=torch.from_numpy(x_seq).permute(0, 2, 1)
y_tensor_ts=torch.from_numpy(y_seq).permute(0, 2, 1)
x_tensor_ts=x_tensor_ts.float()
y_tensor_ts=y_tensor_ts.float()

class TimeSeriesCNN(nn.Module):
    def __init__(self, time_window):
        super(TimeSeriesCNN, self).__init__()
        self.conv1 = nn.Conv1d(in_channels=12, out_channels=32, kernel_size=time_window, padding=time_window//2)
        self.conv2 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=time_window, padding=time_window//2)
        self.fc1 = nn.Linear(64 * 12246, 256)
        self.fc2 = nn.Linear(256, 14 * 12246)

    def forward(self, x):
        x = self.conv1(x)
        x = torch.relu(x)
        x = self.conv2(x)
        x = torch.relu(x)
        x = x.view(x.size(0), -1)  # Flatten for fully connected layer
        x = self.fc1(x)
        x = torch.relu(x)
        x = self.fc2(x)
        x = x.view(x.size(0), 14, 12246)  # Reshape to (batch_size, 14, 12246) to match y_tensor shape
        return x

time_window=5
dataset=TensorDataset(x_tensor_ts,y_tensor_ts)
dataloader=DataLoader(dataset,batch_size=32,shuffle=True)

model_cnn2=TimeSeriesCNN(time_window)
criterion=nn.MSELoss()
optimizer=optim.Adam(model_cnn2.parameters(), lr=0.001,weight_decay=1e-5)

epochs = 10
train_losses_ts=[]
for epoch in range(epochs):
    model_cnn2.train()  # Set model to training mode

    for batch_x, batch_y in dataloader:
        optimizer.zero_grad()  # Clear the gradients

        outputs = model_cnn2(batch_x)  # Forward pass

        loss = criterion(outputs, batch_y)  # Calculate loss

        loss.backward()  # Backward pass (compute gradients)

        optimizer.step()  # Update weights

    print(f"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}")
    train_losses_ts.append(loss)

train_losses_ts = [tl.cpu().item() for tl in train_losses_ts]  # Convert each tensor to a float

plt.plot(train_losses_ts, label="Training loss")
plt.title("Loss at Epoch")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.legend()
plt.show()